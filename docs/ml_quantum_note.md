# âœ… Machine Learning

## The three fundamental elements of machine learning:

**ML = Data + Model + Learning Mechanism**

To fully cover the entire machine learning lifecycle, you also need:

* **Evaluation & Validation**
* **Deployment & Maintenance**

## âœ… 1ï¸âƒ£ Data

### ğŸ¯ Definition

Raw information is transformed through **preprocessing** and **feature engineering** into a numerical representation that the model can compute â€” namely, a **tensor**.

### ğŸ“ Common Preprocessing Formulas

**Standardization**

```
z = (x - Î¼) / Ïƒ
```

**Normalization**

```
x' = (x - x_min) / (x_max - x_min)
```

**One-Hot Encoding:** Convert categorical variables into binary vectors.

### ğŸ§© Feature Engineering & Dimensionality Reduction

* **Feature Engineering:** Create new, more meaningful features from raw data, or transform existing features.
  - **Dimensionality Reduction:** Techniques like PCA or SVD project high-dimensional data into lower dimensions while preserving key information.
    - **PCA:** Projects data onto directions of maximum variance.
    - **SVD:**

      ```
      M = U Î£ V*
      ```

### ğŸ“¦ Tensor Hierarchy

| Type   | Dimensionality | Example |
|--------|----------------|---------|
| Scalar | 0D             | `5` |
| Vector | 1D             | `x âˆˆ â„^F` |
| Matrix | 2D             | `M âˆˆ â„^{R Ã— C}` |
| Tensor | 3D+            | `T âˆˆ â„^{Dâ‚ Ã— Dâ‚‚ Ã— â€¦ Dâ‚–}` |

### ğŸ“Š Input Tensor Formats for Different Models

| Model | Single Sample | Batch Format |
|------------------------|----------------|-----------------------------|
| **Traditional ML** | `x âˆˆ â„^F` | `X âˆˆ â„^{B Ã— F}` |
| **DNN / MLP** | `x âˆˆ â„^F` | `X âˆˆ â„^{B Ã— F}` |
| **CNN** | `X âˆˆ â„^{C Ã— H Ã— W}` | `X âˆˆ â„^{B Ã— C Ã— H Ã— W}` |
| **GNN** | Node: `X âˆˆ â„^{N Ã— F}`<br>Adjacency: `A âˆˆ {0,1}^{N Ã— N}`<br>Edge list: `E âˆˆ â„¤^{2 Ã— M}`<br>Edge features: `E_feat âˆˆ â„^{M Ã— Fâ‚‘}` | â€” |
| **RNN / LSTM** | `X âˆˆ â„^{L Ã— F}` | `X âˆˆ â„^{B Ã— L Ã— F}` |
| **Transformer / LLM** | `X âˆˆ â„^{L Ã— D_model}` | `X âˆˆ â„^{B Ã— L Ã— D_model}` |

### âš›ï¸ Quantum Machine Learning (QML)

| Type | Format | Description |
|------|--------|-------------|
| **Quantum-Enhanced ML** | `|ÏˆâŸ© âˆˆ â„‚^{2â¿}` or `Ï âˆˆ â„‚^{2â¿ Ã— 2â¿}` | Quantum state vector or density matrix; classical data encoded by basis, angle, or amplitude. |
| **QRC** | `x(t) âˆˆ â„^F` | Only output layer trained; quantum reservoir does not require backpropagation. |
| **QAM** | â€” | Patterns stored in quantum state evolution without gradient descent. |

### ğŸ“š GNN & QML Tensor Structures and Message Passing

#### ğŸ§© GNN Tensor Input & Message Passing

**Components:**

1. Node Features `(num_nodes, feature_dim)`
2. Adjacency Matrix `(num_nodes, num_nodes)` or Edge List `(2, num_edges)`
3. Edge Features `(num_edges, edge_feature_dim)`

**Message Passing:**

- Message: $m_{ij}^{l}$ = Message$(h_{i}^{l}, h_{j}^{l}, e_{ij})$
- Aggregate: $m_{i}^{l}$ = Aggregate$({m_{ij}^{l} : j âˆˆ N(i)})$
- Update: $h_i^{l+1}$ = Update$(h_{i}^{l}, m_{i}^{l})$

> GNNs keep graph structure via multiple tensors and message passing.

#### ğŸ§¬ QML Tensor Supplement

- **Qubit:** `|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©`, `|Î±|Â² + |Î²|Â² = 1`
- **Density Matrix:** `Ï = |ÏˆâŸ©âŸ¨Ïˆ|` (pure) or `Ï = Î£ páµ¢|Ïˆáµ¢âŸ©âŸ¨Ïˆáµ¢|` (mixed)
- **Quantum Graph States:**

  
  $\ket{G} = \Pi U_{z}^{\{u,v\}} \ket{+}^{âŠ—n}$
  

- **Encoding:**
  - Basis Encoding
  - Angle Encoding
  - Amplitude Encoding

## âœ… 2ï¸âƒ£ Model

**Definition:** A mathematical function mapping inputs to outputs with trainable parameters.

- Neuron: $a = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$
- Dense: $\text{output} = \text{activation}(\mathbf{XW} + b)$
- Convolution: $\text{output}_{i,j} = \sum_{k,l} \text{input}_{i+k, j+l} \cdot \text{kernel}_{k,l}$
- RNN:

  $\mathbf{h}_t = f(\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{xh} \mathbf{x}_t + b)$
  

  
  $\mathbf{y}_t = g(\mathbf{W}_{hy} \mathbf{h}_t + b)$

- Transformer Attention:

  $\mathbf{Q} = \mathbf{X} \mathbf{W}^Q,\quad 
    \mathbf{K} = \mathbf{X} \mathbf{W}^K,\quad 
    \mathbf{V} = \mathbf{X} \mathbf{W}^V$
  

  
  $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V}$

   - where $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$ are Queryã€Keyã€Value matrices respectively, $\mathbf{W}^Q, \mathbf{W}^K, \mathbf{W}^V$ are weight matrices, $d_k$ is dimension of Key vector.

## âœ… 3ï¸âƒ£ Learning Mechanism

**Definition:** Optimizes parameters to minimize loss.

### ğŸŒ± Core Process

1. Forward: generate predictions.
2. Loss: $L = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$
3. Backpropagation:

  $\frac{\partial L}{\partial w_{ij}} =
   \frac{\partial L}{\partial a_j} \cdot
   \frac{\partial a_j}{\partial z_j} \cdot
   \frac{\partial z_j}{\partial w_{ij}}$

4. Gradient Descent:

  $\mathbf{w}_{n+1} = \mathbf{w}_n - \alpha \nabla L(\mathbf{w}_n)$


5. Adam Optimizer:

  $\begin{aligned}
   \mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
   \mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 \\
   \hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
   \hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1 - \beta_2^t} \\
   \mathbf{w}_t &= \mathbf{w}_{t-1} - \alpha \cdot \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}
   \end{aligned}$

## âœ… 4ï¸âƒ£ Evaluation & Validation

- Accuracy, Precision, Recall, F1, ROC AUC
- Cross-Validation
- Interpretability

## âœ… 5ï¸âƒ£ Deployment & Maintenance

- Deploy via APIs
- Monitor data drift, model degradation
- Retrain when needed
- Use GPU / TPU / FPGA acceleration

## ğŸ”‘ Summary

> **Everything starts with tensors, mapped by models, optimized through learning mechanisms, and wrapped up with evaluation and deployment.**

## ğŸ“– Keywords

Tensor | Message Passing | Adjacency Matrix | Quantum Reservoir Computing (QRC) | Variational Quantum Algorithm (VQA) | Backpropagation | Gradient Descent | Adam Optimizer