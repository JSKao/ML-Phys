# Variational Autoencoder (VAE)
## åŸç†ã€æ•¸å­¸æ¨å°èˆ‡æ¼”ç®—æ³•ç´°ç¯€æµç¨‹

1. VAE åŸç†æ¦‚è¿°
VAE æ˜¯ä¸€ç¨®ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å­¸ç¿’è³‡æ–™çš„æ½›åœ¨ï¼ˆlatentï¼‰çµæ§‹ï¼Œä¸¦èƒ½ç”Ÿæˆæ–°è³‡æ–™ã€‚
å®ƒçµåˆäº†è‡ªç·¨ç¢¼å™¨ï¼ˆAutoencoderï¼‰èˆ‡è®Šåˆ†æ¨æ–·ï¼ˆVariational Inferenceï¼‰ã€‚
ç›®æ¨™ï¼šå­¸ç¿’ $p(x)$ï¼Œå³è³‡æ–™ $x$ çš„ç”Ÿæˆåˆ†å¸ƒã€‚

2. æ¨¡å‹æ¶æ§‹
Encoderï¼ˆæ¨æ–·ç¶²è·¯ï¼‰ï¼š$q_\phi(z|x)$ï¼Œå°‡è³‡æ–™ $x$ æ˜ å°„åˆ°æ½›åœ¨è®Šæ•¸ $z$ çš„åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œ$\mu(x), \sigma(x)$ ç”±ç¥ç¶“ç¶²è·¯çµ¦å‡ºï¼‰ã€‚
Decoderï¼ˆç”Ÿæˆç¶²è·¯ï¼‰ï¼š$p_\theta(x|z)$ï¼Œå¾ $z$ ç”Ÿæˆè³‡æ–™ $x$ï¼ˆé€šå¸¸ä¹Ÿæ˜¯ç¥ç¶“ç¶²è·¯ï¼‰ã€‚

3. æ•¸å­¸æ¨å°
(1) Evidence Lower Bound (ELBO)
æˆ‘å€‘æƒ³æœ€å¤§åŒ– $\log p_\theta(x)$ï¼Œä½†é€™å€‹ç©åˆ†é€šå¸¸ä¸å¯è§£ï¼š

$$ \log p_\theta(x) = \log \int p_\theta(x|z) p(z) dz $$

å¼•å…¥è¿‘ä¼¼å¾Œé©— $q_\phi(z|x)$ï¼Œæœ‰ï¼š

$$ \log p_\theta(x) = \mathbb{E}{q\phi(z|x)} \left[ \log \frac{p_\theta(x|z)p(z)}{q_\phi(z|x)} \right] + D_{KL}(q_\phi(z|x) | p_\theta(z|x)) $$

å…¶ä¸­ $D_{KL}$ æ˜¯ KL divergenceï¼Œéè² ã€‚å› æ­¤ï¼š

$$ \log p_\theta(x) \geq \mathbb{E}{q\phi(z|x)} [\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) | p(z)) $$

é€™å€‹ä¸‹ç•Œç¨±ç‚º ELBOï¼Œæˆ‘å€‘æœ€å¤§åŒ–å®ƒã€‚

(2) VAE çš„ Loss Function
å°å–®ä¸€è³‡æ–™ $x$ï¼ŒVAE çš„ loss ç‚ºï¼š

$$ \mathcal{L}{VAE} = -\mathbb{E}{q_\phi(z|x)} [\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x) | p(z)) $$

ç¬¬ä¸€é …ï¼šé‡å»ºèª¤å·®ï¼ˆreconstruction lossï¼‰ï¼Œè¡¡é‡ decoder ç”Ÿæˆ $x$ çš„èƒ½åŠ›ã€‚
ç¬¬äºŒé …ï¼šKL lossï¼Œè®“ encoder è¼¸å‡ºçš„ $q_\phi(z|x)$ ä¸è¦åé›¢ prior $p(z)$ï¼ˆé€šå¸¸æ˜¯æ¨™æº–é«˜æ–¯ï¼‰ã€‚

(3) é‡åƒæ•¸åŒ–æŠ€å·§ï¼ˆReparameterization Trickï¼‰
$q_\phi(z|x) = N(z; \mu(x), \sigma^2(x))$
ç›´æ¥ sampling $z$ ä¸å¯å¾®åˆ†ï¼Œç„¡æ³•åå‘å‚³æ’­ã€‚
è§£æ³•ï¼š$z = \mu(x) + \sigma(x) \cdot \epsilon$ï¼Œ$\epsilon \sim N(0,1)$
é€™æ¨£ $z$ å° $\mu, \sigma$ å¯å¾®åˆ†ï¼Œloss å¯ä»¥å‚³å› encoderã€‚

4. VAE è¨“ç·´æµç¨‹ï¼ˆPseudocodeï¼‰
è¼¸å…¥è³‡æ–™ $x$
Encoder è¼¸å‡º $\mu(x), \log \sigma^2(x)$
Sampling $\epsilon \sim N(0,1)$ï¼Œè¨ˆç®— $z = \mu + \sigma \cdot \epsilon$
Decoder ç”¨ $z$ ç”Ÿæˆ $\hat{x} = p_\theta(x|z)$
è¨ˆç®— lossï¼š
é‡å»ºèª¤å·®ï¼š$-\log p_\theta(x|z)$ï¼ˆé€šå¸¸ç”¨ MSE æˆ– BCEï¼‰
KL lossï¼š$D_{KL}(N(\mu, \sigma^2) | N(0,1)) = \frac{1}{2} \sum (1 + \log \sigma^2 - \mu^2 - \sigma^2)$
ç¸½ loss = é‡å»ºèª¤å·® + KL loss
åå‘å‚³æ’­ï¼Œæ›´æ–° encoder/decoder åƒæ•¸


5. VAE Loss çš„æ•¸å­¸ç´°ç¯€
KL divergence between $N(\mu, \sigma^2)$ and $N(0,1)$ï¼š

$$ D_{KL}(N(\mu, \sigma^2) | N(0,1)) = \frac{1}{2} \sum_{i=1}^d \left( \mu_i^2 + \sigma_i^2 - \log \sigma_i^2 - 1 \right) $$


x â”€â”€â–º Encoder â”€â”€â–º Î¼, logÏƒÂ² â”€â”€â–º z = Î¼ + ÏƒÂ·Îµ â”€â”€â–º Decoder â”€â”€â–º xÌ‚
         â–²                                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ loss â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


7. VAE ç”Ÿæˆæ–°è³‡æ–™
è¨“ç·´å¥½å¾Œï¼Œç›´æ¥å¾ $N(0,1)$ sampling $z$ï¼Œä¸Ÿçµ¦ decoder ç”¢ç”Ÿæ–° $x$ã€‚


8. ç°¡å–® PyTorch é¢¨æ ¼ç¨‹å¼ç‰‡æ®µ

import torch
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, ...):
        ...
    def forward(self, x):
        mu = ...
        logvar = ...
        return mu, logvar

class Decoder(nn.Module):
    def __init__(self, ...):
        ...
    def forward(self, z):
        x_hat = ...
        return x_hat

def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + std * eps

# è¨“ç·´æ­¥é©Ÿ
mu, logvar = encoder(x)
z = reparameterize(mu, logvar)
x_hat = decoder(z)
recon_loss = F.mse_loss(x_hat, x)
kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
loss = recon_loss + kl_loss
loss.backward()


# Graph Neural Network(GNN)

åœ–ç¥ç¶“ç¶²çµ¡ï¼ˆGraph Neural Network, GNNï¼‰æ˜¯ä¸€ç¨®èƒ½å¤ è™•ç†åœ–çµæ§‹æ•¸æ“šçš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ã€‚èˆ‡å‚³çµ±çš„ç¥ç¶“ç¶²çµ¡ï¼ˆå¦‚ CNN è™•ç†å½±åƒã€RNN è™•ç†åºåˆ—ï¼‰ç›¸æ¯”ï¼ŒGNN çš„è¨­è¨ˆç›®çš„æ˜¯è§£æ±ºç¯€é»ä¹‹é–“éçµæ§‹åŒ–é€£æ¥ï¼ˆå¦‚ç¤¾äº¤ç¶²çµ¡ã€çŸ¥è­˜åœ–è­œã€åˆ†å­çµæ§‹ï¼‰æ‰€å¸¶ä¾†çš„å­¸ç¿’æŒ‘æˆ°ã€‚

---

## ğŸ” ä¸€ã€GNN çš„åŸºæœ¬åŸç†

### ğŸ“Œ æ ¸å¿ƒä»»å‹™ï¼šä¿¡æ¯å‚³éï¼ˆMessage Passingï¼‰

GNN çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¯å€‹ç¯€é»ä¸åªçœ‹è‡ªå·±ï¼Œé‚„æœƒè€ƒæ…®é„°å±…ç¯€é»çš„è³‡è¨Šï¼Œä¾†æ›´æ–°è‡ªå·±çš„è¡¨ç¤ºï¼ˆembeddingï¼‰ã€‚

### ğŸ§± ä¸€èˆ¬ GNN çš„æ›´æ–°æµç¨‹ï¼š

è¨­åœ–ç‚º \( G=(V,E) \)ï¼Œå…¶ä¸­ï¼š

- \( V \)ï¼šç¯€é»é›†åˆï¼ˆnodesï¼‰
- \( E \)ï¼šé‚Šé›†åˆï¼ˆedgesï¼‰
- \( h_v^{(k)} \)ï¼šç¯€é» \( v \) åœ¨ç¬¬ \( k \) å±¤çš„ç‰¹å¾µè¡¨ç¤ºï¼ˆembeddingï¼‰

æ¯ä¸€å±¤ GNN çš„æ­¥é©ŸåŒ…å«ï¼š

1. **Message aggregationï¼ˆè³‡è¨Šå½™æ•´ï¼‰**ï¼š

\[
m_v^{(k)} = \text{AGGREGATE}^{(k)}(\{ h_u^{(k-1)} : u \in N(v) \})
\]

å…¶ä¸­ \( N(v) \) æ˜¯ç¯€é» \( v \) çš„é„°å±…é›†åˆã€‚  
Aggregate å¯ä»¥æ˜¯ mean, sum, max, attention ç­‰ã€‚

2. **Updateï¼ˆç‹€æ…‹æ›´æ–°ï¼‰**ï¼š

\[
h_v^{(k)} = \text{UPDATE}^{(k)}(h_v^{(k-1)}, m_v^{(k)})
\]

é‡è¤‡é€™å€‹æµç¨‹ \( K \) æ¬¡å¾Œï¼Œå°±èƒ½å­¸å‡ºæ¯å€‹ç¯€é»çš„æœ€çµ‚è¡¨ç¤º \( h_v^{(K)} \)ï¼Œå¯ä»¥ç”¨ä¾†åšï¼š

- ç¯€é»åˆ†é¡
- åœ–åˆ†é¡ï¼ˆå¦‚æ•´å€‹åˆ†å­æ˜¯å¦æœ‰æ¯’ï¼‰
- é‚Šé æ¸¬ï¼ˆå¦‚çŸ¥è­˜åœ–è­œè£œå…¨ï¼‰

---

## ğŸ§  äºŒã€èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ¨¡å‹å°æ¯”

| ç‰¹æ€§       | CNNï¼ˆå·ç©ç¥ç¶“ç¶²çµ¡ï¼‰         | RNNï¼ˆå¾ªç’°ç¥ç¶“ç¶²çµ¡ï¼‰      | GNNï¼ˆåœ–ç¥ç¶“ç¶²çµ¡ï¼‰            |
|------------|-----------------------------|--------------------------|-----------------------------|
| è³‡æ–™çµæ§‹   | ç¶²æ ¼ï¼ˆå¦‚å½±åƒï¼‰              | åºåˆ—ï¼ˆå¦‚èªéŸ³ã€æ–‡å­—ï¼‰      | åœ–çµæ§‹ï¼ˆä»»æ„é€£æ¥é—œä¿‚ï¼‰       |
| é„°è¿‘æ€§çµæ§‹ | å›ºå®šé„°åŸŸï¼ˆå¦‚ 3x3 kernelï¼‰   | ç·šæ€§é„°å±…ï¼ˆå‰ä¸€å€‹/ä¸‹ä¸€å€‹ï¼‰ | ä»»æ„é„°å±…ï¼ˆéç·šæ€§ï¼‰           |
| åƒæ•¸å…±ç”¨   | æ˜¯                          | æ˜¯                       | æ˜¯ï¼ˆæ¶ˆæ¯å‚³éå…±ç”¨è¦å‰‡ï¼‰       |
| è³‡è¨Šå‚³éæ–¹å¼ | å·ç©                       | æ™‚åºå‚³é                  | èšåˆé„°å±…ç‰¹å¾µå¾Œæ›´æ–°           |
| èˆ‡ GNN æœ€æ¥è¿‘çš„æ˜¯ï¼Ÿ | ğŸ” CNNï¼ˆæ¦‚å¿µä¸Šæœ€é¡ä¼¼ï¼‰ |                          |                             |

ğŸ”¸ GNN å’Œ CNN æœ€åƒï¼šéƒ½å¼·èª¿ã€Œé„°è¿‘è³‡æ–™çš„èšåˆã€ï¼Œåªæ˜¯ CNN æ˜¯å›ºå®šçµæ§‹ï¼Œè€Œ GNN å¯è™•ç†ä»»æ„åœ–ã€‚

---

## ğŸ§ª ä¸‰ã€å¸¸è¦‹ GNN æ¨¡å‹é¡å‹

| æ¨¡å‹      | èšåˆæ–¹å¼                 | ç‰¹é»                       |
|-----------|--------------------------|----------------------------|
| GCN       | é„°å±…å¹³å‡ + ç·šæ€§è½‰æ›       | æœ€ç¶“å…¸ã€ç°¡å–®               |
| GraphSAGE | Sum/Mean/Max + MLP        | å¯è™•ç†å¤§åœ–ã€æ”¯æ´ inductive |
| GAT       | åŠ å…¥ self-attention æ©Ÿåˆ¶  | è‡ªé©æ‡‰åŠ æ¬Šé„°å±…ï¼Œæå‡æ•ˆæœ   |
| GIN       | ä½¿ç”¨ injective function   | ç†è«–è­‰æ˜æœ‰æ›´å¼·è¡¨ç¤ºèƒ½åŠ›     |

---

## ğŸ”¬ å››ã€GNN çš„æ‡‰ç”¨å ´æ™¯

- ç¤¾äº¤ç¶²çµ¡ï¼šæœ‹å‹æ¨è–¦ã€ç¤¾ç¾¤åµæ¸¬
- åˆ†å­åœ–åˆ†æï¼šè—¥ç‰©è¨­è¨ˆã€æ¯’æ€§é æ¸¬
- çŸ¥è­˜åœ–è­œï¼šæ¨ç†ã€å•ç­”ç³»çµ±
- ç¨‹å¼åˆ†æï¼šéœæ…‹åˆ†æã€ç¨‹å¼ç›¸ä¼¼åº¦
- é‡å­ç‰©ç†ï¼šåˆ†å­è¡¨ç¤ºå­¸ç¿’ã€Hamiltonian ç¶²çµ¡å»ºæ¨¡

---

## ğŸ›  äº”ã€ç°¡å–® PyTorch Geometric ç¯„ä¾‹

```python
import torch
from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class SimpleGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x
