# âœ… Core Structure of Machine Learning and Tensor Formats

## ğŸ“Œ Core Three Elements and Extended Stages

The three fundamental elements of machine learning are:

**ML = Data + Model + Learning Mechanism**

To fully cover the entire machine learning lifecycle, you also need:

* **Evaluation & Validation**
* **Deployment & Maintenance**

## âœ… 1ï¸âƒ£ Data

### ğŸ¯ Definition

Raw information is transformed through **preprocessing** and **feature engineering** into a numerical representation that the model can compute â€” namely, a **tensor**.

### ğŸ“ Common Preprocessing Formulas

**Standardization**

```
z = (x - Î¼) / Ïƒ
```

**Normalization**

```
x' = (x - x_min) / (x_max - x_min)
```

**One-Hot Encoding:** Convert categorical variables into binary vectors.

### ğŸ§© Feature Engineering & Dimensionality Reduction

* **Feature Engineering:** Create new, more meaningful features from raw data, or transform existing features.
  - **Dimensionality Reduction:** Techniques like PCA or SVD project high-dimensional data into lower dimensions while preserving key information.
    - **PCA:** Projects data onto directions of maximum variance.
    - **SVD:**

      ```
      M = U Î£ V*
      ```

### ğŸ“¦ Tensor Hierarchy

| Type   | Dimensionality | Example |
|--------|----------------|---------|
| Scalar | 0D             | `5` |
| Vector | 1D             | `x âˆˆ â„^F` |
| Matrix | 2D             | `M âˆˆ â„^{R Ã— C}` |
| Tensor | 3D+            | `T âˆˆ â„^{Dâ‚ Ã— Dâ‚‚ Ã— â€¦ Dâ‚–}` |

### ğŸ“Š Input Tensor Formats for Different Models

| Model | Single Sample | Batch Format |
|------------------------|----------------|-----------------------------|
| **Traditional ML** | `x âˆˆ â„^F` | `X âˆˆ â„^{B Ã— F}` |
| **DNN / MLP** | `x âˆˆ â„^F` | `X âˆˆ â„^{B Ã— F}` |
| **CNN** | `X âˆˆ â„^{C Ã— H Ã— W}` | `X âˆˆ â„^{B Ã— C Ã— H Ã— W}` |
| **GNN** | Node: `X âˆˆ â„^{N Ã— F}`<br>Adjacency: `A âˆˆ {0,1}^{N Ã— N}`<br>Edge list: `E âˆˆ â„¤^{2 Ã— M}`<br>Edge features: `E_feat âˆˆ â„^{M Ã— Fâ‚‘}` | â€” |
| **RNN / LSTM** | `X âˆˆ â„^{L Ã— F}` | `X âˆˆ â„^{B Ã— L Ã— F}` |
| **Transformer / LLM** | `X âˆˆ â„^{L Ã— D_model}` | `X âˆˆ â„^{B Ã— L Ã— D_model}` |

### âš›ï¸ Quantum Machine Learning (QML)

| Type | Format | Description |
|------|--------|-------------|
| **Quantum-Enhanced ML** | `|ÏˆâŸ© âˆˆ â„‚^{2â¿}` or `Ï âˆˆ â„‚^{2â¿ Ã— 2â¿}` | Quantum state vector or density matrix; classical data encoded by basis, angle, or amplitude. |
| **QRC** | `x(t) âˆˆ â„^F` | Only output layer trained; quantum reservoir does not require backpropagation. |
| **QAM** | â€” | Patterns stored in quantum state evolution without gradient descent. |

### ğŸ“š GNN & QML Tensor Structures and Message Passing

#### ğŸ§© GNN Tensor Input & Message Passing

**Components:**

1. Node Features `(num_nodes, feature_dim)`
2. Adjacency Matrix `(num_nodes, num_nodes)` or Edge List `(2, num_edges)`
3. Edge Features `(num_edges, edge_feature_dim)`

**Message Passing:**

- Message: `m_ij^(l) = Message(h_i^(l), h_j^(l), e_ij)`
- Aggregate: `m_i^(l) = Aggregate({m_ij^(l) : j âˆˆ N(i)})`
- Update: `h_i^(l+1) = Update(h_i^(l), m_i^(l))`

> GNNs keep graph structure via multiple tensors and message passing.

#### ğŸ§¬ QML Tensor Supplement

- **Qubit:** `|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©`, `|Î±|Â² + |Î²|Â² = 1`
- **Density Matrix:** `Ï = |ÏˆâŸ©âŸ¨Ïˆ|` (pure) or `Ï = Î£ páµ¢|Ïˆáµ¢âŸ©âŸ¨Ïˆáµ¢|` (mixed)
- **Quantum Graph States:**

  ```
  |GâŸ© = âˆ U_z(u,v) |+âŸ©^{âŠ—n}
  ```

- **Encoding:**
  - Basis Encoding
  - Angle Encoding
  - Amplitude Encoding

## âœ… 2ï¸âƒ£ Model

**Definition:** A mathematical function mapping inputs to outputs with trainable parameters.

- Neuron: `a = Ïƒ(w â‹… x + b)`
- Dense: `output = activation(XW + b)`
- Convolution: `output_ij = Î£ input_{i+k,j+l} â‹… kernel_{kl}`
- RNN:

  ```
  h_t = f(W_hh h_{t-1} + W_xh x_t + b)
  y_t = g(W_hy h_t + b)
  ```

- Transformer Attention:

  ```
  Q = XW^Q, K = XW^K, V = XW^V
  Attention(Q,K,V) = softmax(QKáµ—/âˆšd_k) V
  ```

## âœ… 3ï¸âƒ£ Learning Mechanism

**Definition:** Optimizes parameters to minimize loss.

### ğŸŒ± Core Process

1. Forward: generate predictions.
2. Loss: `L = (1/N) Î£ (yáµ¢ - Å·áµ¢)Â²`
3. Backpropagation:

  ```
  âˆ‚L/âˆ‚w_ij = âˆ‚L/âˆ‚a_j â‹… âˆ‚a_j/âˆ‚z_j â‹… âˆ‚z_j/âˆ‚w_ij
  ```

4. Gradient Descent:

  ```
  w_{n+1} = w_n - Î± âˆ‡L(w_n)
  ```

5. Adam Optimizer:

  ```
  m_t = Î²â‚ m_{t-1} + (1-Î²â‚)g_t
  v_t = Î²â‚‚ v_{t-1} + (1-Î²â‚‚)g_tÂ²
  mÌ‚_t = m_t / (1 - Î²â‚áµ—)
  vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)
  w_t = w_{t-1} - Î± mÌ‚_t / (âˆšvÌ‚_t + Îµ)
  ```

## âœ… 4ï¸âƒ£ Evaluation & Validation

- Accuracy, Precision, Recall, F1, ROC AUC
- Cross-Validation
- Interpretability

## âœ… 5ï¸âƒ£ Deployment & Maintenance

- Deploy via APIs
- Monitor data drift, model degradation
- Retrain when needed
- Use GPU / TPU / FPGA acceleration

## ğŸ”‘ Summary

> **Everything starts with tensors, mapped by models, optimized through learning mechanisms, and wrapped up with evaluation and deployment.**

## ğŸ“– Keywords

Tensor | Message Passing | Adjacency Matrix | Quantum Reservoir Computing (QRC) | Variational Quantum Algorithm (VQA) | Backpropagation | Gradient Descent | Adam Optimizer
