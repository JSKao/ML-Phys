# âœ… æ©Ÿå™¨å­¸ç¿’æ ¸å¿ƒæ§‹æˆèˆ‡å¼µé‡æ ¼å¼æ•´ç†

---

## ğŸ“Œ æ ¸å¿ƒä¸‰è¦ç´ èˆ‡æ“´å±•ç’°ç¯€

æ©Ÿå™¨å­¸ç¿’æœ€æ ¸å¿ƒçš„ä¸‰å¤§å…ƒç´ æ˜¯ï¼š

**ML = è³‡æ–™ (Data) + æ¨¡å‹ (Model) + å­¸ç¿’æ©Ÿåˆ¶ (Learning Mechanism)**

è‹¥è¦å®Œæ•´è¦†è“‹ ML ç³»çµ±çš„ç”Ÿå‘½é€±æœŸï¼Œå¯å†åŠ ä¸Šï¼š

* **è©•ä¼°èˆ‡é©—è­‰ (Evaluation & Validation)**
* **éƒ¨ç½²èˆ‡ç¶­è­· (Deployment & Maintenance)**

---

## âœ… ä¸€ã€è³‡æ–™ (Data)

### ğŸ¯ å®šç¾©

åŸå§‹è³‡è¨Šç¶“é **é è™•ç†**ã€**ç‰¹å¾µå·¥ç¨‹**ï¼Œæœ€çµ‚è½‰æ›ç‚ºæ¨¡å‹å¯è¨ˆç®—çš„ **æ•¸å€¼è¡¨ç¤º**ï¼Œä¹Ÿå°±æ˜¯ **å¼µé‡ (Tensor)**ã€‚

---

### ğŸ“ å¸¸è¦‹é è™•ç†å…¬å¼

**æ¨™æº–åŒ– (Standardization)**


$z = \frac{x - \mu}{\sigma}$


**æ­£è¦åŒ– (Normalization)**


$x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$

**ç¨ç†±ç·¨ç¢¼ (One-Hot Encoding)**ï¼šå°‡é¡åˆ¥è®Šæ•¸è½‰ç‚ºäºŒé€²ä½å‘é‡ã€‚

---

### ï¿½ ç‰¹å¾µå·¥ç¨‹èˆ‡é™ç¶­è£œå……

* **ç‰¹å¾µå·¥ç¨‹ (Feature Engineering)**ï¼šå¾åŸå§‹è³‡æ–™ä¸­å»ºç«‹æ–°çš„ã€æ›´æœ‰æ„ç¾©çš„ç‰¹å¾µï¼Œæˆ–å°ç¾æœ‰ç‰¹å¾µé€²è¡Œè½‰æ›ã€‚
  - **é™ç¶­ (Dimensionality Reduction)**ï¼šå¦‚ä¸»æˆåˆ†åˆ†æ (PCA) æˆ–å¥‡ç•°å€¼åˆ†è§£ (SVD)ï¼Œå°‡é«˜ç¶­è³‡æ–™æŠ•å½±åˆ°ä½ç¶­ç©ºé–“ï¼ŒåŒæ™‚ä¿ç•™é‡è¦è³‡è¨Šã€‚
    - **PCA**ï¼šå°‡è³‡æ–™æŠ•å½±åˆ°æœ€å¤§è®Šç•°æ–¹å‘ã€‚
    - **SVD** æ•¸å­¸è¡¨ç¤ºï¼š
      $$
      \mathbf{M} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^*
      $$
      å…¶ä¸­ $\mathbf{M}$ æ˜¯åŸå§‹è³‡æ–™çŸ©é™£ï¼Œ$\mathbf{U}$ å’Œ $\mathbf{V}^*$ æ˜¯ä¹ˆæ­£çŸ©é™£ï¼Œ$\mathbf{\Sigma}$ æ˜¯å¥‡ç•°å€¼å°è§’çŸ©é™£ã€‚


### ï¿½ğŸ“¦ å¼µé‡çš„éšå±¤

| é¡å‹          | ç¶­åº¦  | ä¾‹å­                                                                 |
| ----------- | --- | ------------------------------------------------------------------ |
| æ¨™é‡ (Scalar) | 0D  | $5$                                                              |
| å‘é‡ (Vector) | 1D  | $\mathbf{x} \in \mathbb{R}^F$                                    |
| çŸ©é™£ (Matrix) | 2D  | $\mathbf{M} \in \mathbb{R}^{R \times C}$                         |
| å¼µé‡ (Tensor) | 3D+ | $\mathbf{T} \in \mathbb{R}^{D_1 \times D_2 \times \dots D_k}$ |


### ğŸ“Š ä¸åŒæ¨¡å‹çš„è¼¸å…¥å¼µé‡æ ¼å¼

| æ¨¡å‹                         | å–®æ¨£æœ¬                                                                                                                                                                                                                                   | æ‰¹æ¬¡                                                           |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------ |
| **å‚³çµ± ML (è¿´æ­¸/SVM/K-Means)** | $\mathbf{x} \in \mathbb{R}^F$                                                                                                                                                                                                       | $\mathbf{X} \in \mathbb{R}^{B \times F}$                   |
| **DNN / MLP**              | $\mathbf{x} \in \mathbb{R}^F$                                                                                                                                                                                                       | $\mathbf{X} \in \mathbb{R}^{B \times F}$                   |
| **CNN**                    | $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$                                                                                                                                                                                   | $\mathbf{X} \in \mathbb{R}^{B \times C \times H \times W}$ |
| **GNN**                    | Node: $\mathbf{X} \in \mathbb{R}^{N \times F}$<br>Adjacency: $\mathbf{A} \in {0,1}^{N \times N}$<br>Edge list: $\mathbf{E} \in \mathbb{Z}^{2 \times M}$<br>Edge features: $\mathbf{E}_{feat} \in \mathbb{R}^{M \times F_e}$ | â€”                                                            |
| **RNN / LSTM**             | $\mathbf{X} \in \mathbb{R}^{L \times F}$                                                                                                                                                                                            | $\mathbf{X} \in \mathbb{R}^{B \times L \times F}$          |
| **Transformer / LLM**      | Token: $\mathbf{X} \in \mathbb{R}^{L \times D_{model}}$                                                                                                                                                                            | $\mathbf{X} \in \mathbb{R}^{B \times L \times D_{model}}$ |

---

### âš›ï¸ é‡å­æ©Ÿå™¨å­¸ç¿’ (QML)

| é¡å‹                                    | æ ¼å¼                                 | æè¿°                                                                             |                            |
| ------------------------------------- | ---------------------------------- | ------------------------------------------------------------------------------ | -------------------------- |
| **é‡å­å¢å¼·å‹ ML**                          | $\ket{\psi} \in \mathbb{C}^{2^n}$ æˆ– $\rho \in \mathbb{C}^{2^n \times 2^n}$  | é‡å­æ…‹å‘é‡æˆ–å¯†åº¦çŸ©é™£ï¼Œç¶“å…¸è³‡æ–™å¯ç”¨åŸºåº•ç·¨ç¢¼ã€è§’åº¦ç·¨ç¢¼ã€æŒ¯å¹…ç·¨ç¢¼ç­‰æ–¹å¼åµŒå…¥ |  |
| **Quantum Reservoir Computing (QRC)** | $\mathbf{x}(t) \in \mathbb{R}^F$ | åªè¨“ç·´è¼¸å‡ºå±¤ï¼Œå…§éƒ¨é‡å­æ…‹ä¸éœ€åå‘å‚³æ’­                                                             |                            |
| **Quantum Associative Memory (QAM)**  | â€”                                  | æ¨¡å¼å­˜æ–¼é‡å­æ…‹æ¼”åŒ–ï¼Œä¸éœ€æ¢¯åº¦ä¸‹é™                                                               |                            |

### ğŸ“š GNN & QML Tensor Structures and Message Passing (è£œå……)

#### ğŸ§© GNN Tensor Input & Message Passing

* **Input Structure**: Graphs are non-Euclidean data, usually without fixed order or grid.
* **Input Components**:
  1. **Node Feature Matrix (X)**
     * Shape: `(num_nodes, feature_dimension)`
     * Each row is a node feature vector.
  2. **Adjacency Matrix (A)** or **Edge List**
     * Adjacency: `(num_nodes, num_nodes)`
     * Edge list (PyTorch Geometric): `(2, num_edges)`, source/target indices.
  3. **Edge Feature Matrix (E)** (if edges have attributes)
     * Shape: `(num_edges, edge_feature_dimension)`

* **Message Passing Mechanism**:
  - Each node builds messages from itself, neighbors, and edge features.
  - Nodes aggregate neighbors' messages (sum/mean/max).
  - Node updates its representation based on aggregated messages.

* **Message Passing Formulas:**
  - è¨Šæ¯å»ºç«‹ (Message construction):
    $$
    m_{ij}^{(l)} = \text{Message}(h_i^{(l)}, h_j^{(l)}, e_{ij})
    $$
  - è¨Šæ¯èšåˆ (Message aggregation):
    $$
    m_i^{(l)} = \text{Aggregate}({m_{ij}^{(l)} : j \in N(i)})
    $$
  - ç¯€é»æ›´æ–° (Node update):
    $$
    h_i^{(l+1)} = \text{Update}(h_i^{(l)}, m_i^{(l)})
    $$

> GNNs do not flatten the whole graph into a single vector, but use multiple tensors and message passing to capture structure.

#### ğŸ§¬ QML Tensor Structure Supplement

* **Quantum-Enhanced ML**
  * Encoded state vector: $|\psi\rangle \in \mathbb{C}^{2^n}$
  * Density matrix: $\rho \in \mathbb{C}^{2^n \times 2^n}$
  * Encoding: basis, angle, amplitude

* **Quantum Reservoir Computing (QRC)**
  * Input: $\mathbf{x}(t) \in \mathbb{R}^F$
  * Only output layer is trained; internal quantum state evolution does not require backpropagation.

* **Quantum Associative Memory (QAM)**
  * Patterns are stored in quantum state evolution, no gradient descent required.

#### é‡å­è³‡æ–™çš„ç†è«–è¡¨ç¤ºèˆ‡ç·¨ç¢¼è£œå……

* **é‡å­ä½å…ƒ (Qubit)**ï¼šé‡å­è³‡è¨Šçš„åŸºæœ¬å–®ä½ã€‚å–®ä¸€ qubit ç´”æ…‹ï¼š
  $|\psi\rangle = \alpha |0\rangle + \beta |1\rangle,\quad \alpha, \beta \in \mathbb{C},\ |\alpha|^2 + |\beta|^2 = 1$
  å¤š qubit ç³»çµ±ç‹€æ…‹ä»¥å¼µé‡ç©çµ„åˆã€‚

* **å¯†åº¦çŸ©é™£ (Density Matrix)**ï¼šæè¿°ç´”æ…‹æˆ–æ··åˆæ…‹ã€‚n qubit ç³»çµ±å¯†åº¦çŸ©é™£ $\rho$ ç‚º $2^n \times 2^n$ Hermitian çŸ©é™£ï¼Œ$\text{Tr}(\rho) = 1$ã€‚
  - ç´”æ…‹ï¼š$\rho = |\psi\rangle\langle\psi|$
  - æ··åˆæ…‹ï¼š$\rho = \sum_i p_i |\psi_i\rangle\langle\psi_i|$

* **é‡å­åœ–æ…‹ (Quantum Graph States)**ï¼šå°‡æ•¸å­¸åœ–è¡¨ç¤ºç‚ºé‡å­æ…‹ã€‚
  - ç„¡æ¬Šé‡åœ– $G=(V,E)$ï¼š
    
    $|G\rangle = \prod_{(u,v)\in E} U_z(u,v) |+\rangle^{\otimes n}$
    
    å…¶ä¸­ $U_z(u,v)$ ç‚ºå—æ§ Z é–€ã€‚
  - åŠ æ¬Šåœ–ï¼š
    $U_z(u,v,w) = e^{-iw\sigma_z^u \sigma_z^v}$
    
    $\sigma_z$ ç‚º Pauli Z çŸ©é™£ï¼Œw ç‚ºé‚Šæ¬Šé‡ã€‚

* **ç¶“å…¸è³‡æ–™åˆ°é‡å­æ…‹çš„ç·¨ç¢¼ (Classical-to-Quantum Data Encoding)**ï¼š
  - **åŸºåº•ç·¨ç¢¼ (Basis Encoding)**ï¼šå°‡ç¶“å…¸äºŒé€²ä½è³‡æ–™ç›´æ¥æ˜ å°„åˆ° qubit è¨ˆç®—åŸºåº•æ…‹ã€‚
    
    $|b_1 b_2 \ldots b_F\rangle = |b_1\rangle \otimes |b_2\rangle \otimes \ldots \otimes |b_F\rangle$
    
  - **è§’åº¦ç·¨ç¢¼ (Angle Encoding)**ï¼šå°‡è³‡æ–™å€¼ç·¨ç¢¼ç‚ºé‡å­é–€æ—‹è½‰è§’åº¦ã€‚
  - **æŒ¯å¹…ç·¨ç¢¼ (Amplitude Encoding)**ï¼šå°‡è³‡æ–™å‘é‡æ­£è¦åŒ–å¾Œä½œç‚ºé‡å­æ…‹çš„æŒ¯å¹…ã€‚

## âœ… äºŒã€æ¨¡å‹ (Model)

**å®šç¾©**ï¼šä»¥æ•¸å­¸å‡½æ•¸å½¢å¼é€£æ¥è¼¸å…¥åˆ°è¼¸å‡ºï¼ŒåŒ…å«å¯è¨“ç·´åƒæ•¸ã€‚

* **ç¥ç¶“å…ƒ (Neuron)**

  
  $a = \sigma(\mathbf{w} \cdot \mathbf{x} + b)$
  

* **å¯†é›†å±¤ (Dense)**

  
  $\text{output} = \text{activation}(\mathbf{XW} + b)$
  

* **å·ç©å±¤ (CNN)**

  
  $\text{output}_{i,j} = \sum_{k,l} \text{input}_{i+k, j+l} \cdot \text{kernel}_{k,l}$
  

* **å¾ªç’°å±¤ (RNN)**

  
  $\mathbf{h}_t = f(\mathbf{W}_{hh} \mathbf{h}_{t-1} + \mathbf{W}_{xh} \mathbf{x}_t + b)$
  

  
  $\mathbf{y}_t = g(\mathbf{W}_{hy} \mathbf{h}_t + b)$
  

* **Transformer Attention**

  
  $\mathbf{Q} = \mathbf{X} \mathbf{W}^Q,\quad 
  \mathbf{K} = \mathbf{X} \mathbf{W}^K,\quad 
  \mathbf{V} = \mathbf{X} \mathbf{W}^V$
  

  
  $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}} \right) \mathbf{V}$
  

  - å…¶ä¸­ $\mathbf{Q}$, $\mathbf{K}$, $\mathbf{V}$ åˆ†åˆ¥ç‚º Queryã€Keyã€Value çŸ©é™£ï¼Œ$\mathbf{W}^Q, \mathbf{W}^K, \mathbf{W}^V$ ç‚ºæ¬Šé‡çŸ©é™£ï¼Œ$d_k$ ç‚º Key å‘é‡ç¶­åº¦ã€‚

---

## âœ… ä¸‰ã€å­¸ç¿’æ©Ÿåˆ¶ (Learning Mechanism)

**å®šç¾©**ï¼šå„ªåŒ–å¯å­¸åƒæ•¸ä»¥æœ€å°åŒ–æå¤±ã€‚

---

### ğŸŒ± æ ¸å¿ƒæµç¨‹

1ï¸âƒ£ **å‰å‘å‚³æ’­ (Forward Propagation)**
è¼¸å…¥è³‡æ–™ç¶“ç¶²è·¯ç”¢ç”Ÿé æ¸¬ã€‚

2ï¸âƒ£ **æå¤±å‡½æ•¸ (Loss Function)**
è¡¡é‡é æ¸¬èˆ‡çœŸå€¼çš„å·®ç•°ï¼Œä¾‹å¦‚å‡æ–¹èª¤å·®ï¼š


$L = \frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2$


3ï¸âƒ£ **åå‘å‚³æ’­ (Backpropagation)**
ç”¨éˆå¼æ³•å‰‡è¨ˆç®—æ¢¯åº¦ï¼š


$\frac{\partial L}{\partial w_{ij}} =
\frac{\partial L}{\partial a_j} \cdot
\frac{\partial a_j}{\partial z_j} \cdot
\frac{\partial z_j}{\partial w_{ij}}$


4ï¸âƒ£ **æ¢¯åº¦ä¸‹é™ (Gradient Descent)**
æ ¹æ“šæ¢¯åº¦èª¿æ•´åƒæ•¸ï¼š


$\mathbf{w}_{n+1} = \mathbf{w}_n - \alpha \nabla L(\mathbf{w}_n)$


5ï¸âƒ£ **å„ªåŒ–å™¨ (å¦‚ Adam)**
åœ¨åŸºæœ¬æ¢¯åº¦ä¸‹é™ä¸Šå¼•å…¥å‹•é‡ã€è‡ªé©æ‡‰å­¸ç¿’ç‡èˆ‡åå·®ä¿®æ­£ï¼š


$\begin{aligned}
\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \mathbf{g}_t \\
\mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) \mathbf{g}_t^2 \\
\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
\hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1 - \beta_2^t} \\
\mathbf{w}_t &= \mathbf{w}_{t-1} - \alpha \cdot \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon}
\end{aligned}$


---

âœ… **é‡é»**

* åå‘å‚³æ’­ï¼šè¨ˆç®—æ¢¯åº¦çš„æ–¹æ³•
* æ¢¯åº¦ä¸‹é™ï¼šç”¨æ¢¯åº¦æ›´æ–°åƒæ•¸çš„ç­–ç•¥
* Adamï¼šä¸€ç¨®é€²éšçš„æ¢¯åº¦ä¸‹é™è®Šé«”
  â†’ å…±åŒæ§‹æˆè¨“ç·´è¿´åœˆï¼Œç›´åˆ°æ”¶æ–‚ã€‚

---

## âœ… å››ã€è©•ä¼°èˆ‡é©—è­‰ (Evaluation & Validation)

* Accuracy, Precision, Recall, F1, ROC AUC
* Cross-Validation
* å¯è§£é‡‹æ€§ (Interpretability)

---

## âœ… äº”ã€éƒ¨ç½²èˆ‡ç¶­è­· (Deployment & Maintenance)

* ä»¥ API éƒ¨ç½²æ¨¡å‹
* ç›£æ§è³‡æ–™æ¼‚ç§»èˆ‡æ¨¡å‹é€€åŒ–
* éœ€è¦æ™‚é€²è¡Œå†è¨“ç·´
* GPU / TPU / FPGA åŠ é€Ÿ

---

## ğŸ”‘ ç¸½çµ

> **ä¸€åˆ‡å¾å¼µé‡é–‹å§‹ï¼Œä¾é æ¨¡å‹é€²è¡Œæ˜ å°„ï¼Œé€éå­¸ç¿’æ©Ÿåˆ¶å„ªåŒ–ï¼Œæœ€å¾Œä»¥åš´è¬¹çš„è©•ä¼°é©—è­‰èˆ‡æœ‰æ•ˆéƒ¨ç½²æ”¶å°¾ã€‚**

---

## ğŸ“– Keywords

Tensor ï½œ Message Passing ï½œ Adjacency Matrix ï½œ Quantum Reservoir Computing (QRC) ï½œ Variational Quantum Algorithm (VQA) ï½œ Backpropagation ï½œ Gradient Descent ï½œ Adam Optimizer

---

